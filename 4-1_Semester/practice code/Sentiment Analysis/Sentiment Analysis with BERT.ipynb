{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Analysis with BERT.ipynb","provenance":[],"collapsed_sections":["lT4_AQu9Ak7z"],"mount_file_id":"1g8xA64PU8AD5Hq57iIkGiWVsZDN-JYq_","authorship_tag":"ABX9TyOgDw5XQgrgduzkFvKb0fCD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1YMGib7L39TA"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["### import dataset"],"metadata":{"id":"5mCn-WL27_vR"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WP2c1GyYMWyq","executionInfo":{"status":"ok","timestamp":1658033740112,"user_tz":-360,"elapsed":13271,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"b02a6320-e1f5-42e6-9b92-c0038e16aff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import sklearn\n","from tqdm import tqdm\n","df= pd.read_csv('/content/drive/MyDrive/4-1_Semester/practice code/Sentiment Analysis/IMDB Dataset.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"vCit3qL88Joo","executionInfo":{"status":"ok","timestamp":1656873271041,"user_tz":-360,"elapsed":7850,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"f7c05c24-e344-45af-9f5d-b0f0bda6536c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"],"text/html":["\n","  <div id=\"df-e0f3e5f2-e7d1-434e-a4e4-a962a4f33b1f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0f3e5f2-e7d1-434e-a4e4-a962a4f33b1f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e0f3e5f2-e7d1-434e-a4e4-a962a4f33b1f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e0f3e5f2-e7d1-434e-a4e4-a962a4f33b1f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df['sentiment'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"OfPtPRu-p6ho","executionInfo":{"status":"error","timestamp":1658033848834,"user_tz":-360,"elapsed":2539,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"9b39ae73-9220-4cfe-fd87-3a503d83de2d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1be892254d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["%matplotlib inline\n","df['sentiment'].value_counts().plot(kind='bar')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"QH-zb382mjpq","executionInfo":{"status":"error","timestamp":1658033850851,"user_tz":-360,"elapsed":27,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"62758817-fad5-4f4e-a73f-0687f0fb75df"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c9772cba1f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["##install Transformers Library\n","!pip install transformers"],"metadata":{"id":"0Jj4rV4C9yP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the BERT Classifier and Tokenizer along with Input module\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from transformers import InputExample, InputFeatures\n","\n","model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"metadata":{"id":"548_A8oL-RBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"Eu2h3SGz-1z2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### # Change positive/negative into numeric"],"metadata":{"id":"jfCFr5IY_Ie9"}},{"cell_type":"code","source":["df.sentiment = df.sentiment.apply(lambda x: 0 if x =='negative' else 1)\n","df.sample()\n","df.head()"],"metadata":{"id":"4vJOEe0V_KBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sentiment'].value_counts()"],"metadata":{"id":"_HCROcE6pw4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","df['sentiment'].value_counts().plot(kind='bar')"],"metadata":{"id":"RMqBm3BEol5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = df[:45000]\n","test = df[45000:]"],"metadata":{"id":"QAs87IYQ_vlb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"lT4_AQu9Ak7z"}},{"cell_type":"code","source":["##BERT tokenizer examples\n","\n","example='A quick brown fox jumps over the lazy dog'\n","tokens=tokenizer.tokenize(example)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(tokens)\n","print(token_ids)"],"metadata":{"id":"krdLtmW_Ay7p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data processing"],"metadata":{"id":"y45KGzOL__6W"}},{"cell_type":"markdown","source":["1. Add special tokens to separate sentences and do classification\n","2. Pass sequences of constant length (introduce padding)\n","3. Create array of 0s (pad token) and 1s (real token) called attention mask"],"metadata":{"id":"Id_PYKrVAD_K"}},{"cell_type":"code","source":["## convert_data_to_examples: will accept train/test dataset and convert each row into an InputExamples object.\n","def convert_data_to_examples(train, test, review, sentiment): \n","    train_InputExamples = train.apply(lambda x: InputExample(guid=None,\n","                                                          text_a = x[review], \n","                                                          label = x[sentiment]), axis = 1)\n","\n","    test_InputExamples = test.apply(lambda x: InputExample(guid=None,\n","                                                          text_a = x[review], \n","                                                          label = x[sentiment]), axis = 1)\n","  \n","    return train_InputExamples, test_InputExamples\n","\n","train_InputExamples, test_InputExamples = convert_data_to_examples(train,  test, 'review',  'sentiment')"],"metadata":{"id":"mwXFJB57BXfp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #train_InputExamples[0]"],"metadata":{"id":"ANNSTrpoC0YI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n","    features = [] # -> will hold InputFeatures to be converted later\n","\n","    for e in tqdm(examples):\n","        input_dict = tokenizer.encode_plus(\n","            e.text_a,\n","            add_special_tokens=True,    # Add 'CLS' and 'SEP'\n","            max_length=max_length,    # truncates if len(s) > max_length\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n","            truncation=True\n","        )\n","\n","        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],input_dict[\"token_type_ids\"],\n","                                                     input_dict['attention_mask'])\n","        \n","        features.append(InputFeatures( input_ids=input_ids, attention_mask=attention_mask, \n","                                      token_type_ids=token_type_ids, label=e.label) )\n","\n","    def gen():\n","        for f in features:\n","            yield (\n","                {\n","                    \"input_ids\": f.input_ids,\n","                    \"attention_mask\": f.attention_mask,\n","                    \"token_type_ids\": f.token_type_ids,\n","                },\n","                f.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None]),\n","                \"token_type_ids\": tf.TensorShape([None]),\n","            },\n","            tf.TensorShape([]),\n","        ),\n","    )\n","\n","\n","DATA_COLUMN = 'review'\n","LABEL_COLUMN = 'sentiment'"],"metadata":{"id":"yxIigX22E0Np"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert_examples_to_tf_dataset: will tokenize the InputExample objects, then create the required input format with the \n","#                                tokenized objects, finally, create an input dataset that we can feed to the model.\n","\n","train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n","train_data = train_data.shuffle(50).batch(32).repeat(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZTQpYZIFWF8","executionInfo":{"status":"ok","timestamp":1656873622695,"user_tz":-360,"elapsed":290021,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"14dc046e-0ce6-4762-8d7e-d776eb1e8bb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/45000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 45000/45000 [04:50<00:00, 154.89it/s]\n"]}]},{"cell_type":"code","source":["test_data = convert_examples_to_tf_dataset(list(test_InputExamples), tokenizer)\n","test_data = test_data.batch(32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96sw2CVEJkbK","executionInfo":{"status":"ok","timestamp":1656873652617,"user_tz":-360,"elapsed":29932,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"dc6b5ad7-9b92-4f70-b91b-e42446459290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 5000/5000 [00:29<00:00, 166.72it/s]\n"]}]},{"cell_type":"markdown","source":["### Fed to the Model"],"metadata":{"id":"a9ITjn_CKMxW"}},{"cell_type":"code","source":["model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n","\n","model.fit(train_data, epochs=2, validation_data=test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"havaF38AKQ5U","outputId":"07b76a10-42da-4a45-efa6-69a2a9e3015f","executionInfo":{"status":"ok","timestamp":1656878611985,"user_tz":-360,"elapsed":4959382,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2814/2814 [==============================] - 2481s 873ms/step - loss: 0.2399 - accuracy: 0.8993 - val_loss: 0.2814 - val_accuracy: 0.8914\n","Epoch 2/2\n","2814/2814 [==============================] - 2452s 871ms/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.4680 - val_accuracy: 0.8928\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f36c48934d0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### Test Model"],"metadata":{"id":"fUTO87iBhpt_"}},{"cell_type":"code","source":["pred_sentences = ['worst movie of my life, will never watch movies from this series',\n","                  'I was going to say something awesome or great or good, but the movie was so bad',\n","                  'I loved this movie',\n","                  'The first half was so bad, but i loved the 2nd part']"],"metadata":{"id":"X1kEVCfnhsLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')   # we are tokenizing before sending into our trained model\n","\n","tf_outputs = model(tf_batch)                                  \n","\n","tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)       # axis=-1, this means that the index that will be returned by argmax will be taken from the *last* axis.\n","\n","labels = ['Negative','Positive']\n","label = tf.argmax(tf_predictions, axis=1)\n","\n","label = label.numpy()\n","for i in range(len(pred_sentences)):\n","    print(pred_sentences[i], \": \", labels[label[i]])"],"metadata":{"id":"N2ZU-uIZh4Ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656879138384,"user_tz":-360,"elapsed":374,"user":{"displayName":"Shawan Das","userId":"12697085418713095865"}},"outputId":"a3938f6d-749b-4b0e-83f1-c13e219679fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["worst movie of my life, will never watch movies from this series :  Negative\n","I was going to say something awesome or great or good, but the movie was so bad :  Negative\n","I loved this movie :  Positive\n","The first half was so bad, but i loved the 2nd part :  Negative\n"]}]}]}